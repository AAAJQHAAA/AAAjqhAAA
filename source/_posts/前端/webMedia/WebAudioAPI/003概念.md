# Web Audio API背后的基本概念
- [文档地址](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API)
- 本文解释了Web Audio API的功能如何工作的背后的一些音频理论，以帮助您在设计应用如何路由音频时做出明智的决策。如果你还不是一个声音工程师，它将给给予你足够的背景，以了解为什么网络音频API的工作方式。
# 音频图
- Web Audio API涉及在音频上下文中处理音频操作，并且被设计为允许模块化路由。每个音频节点执行基本的音频操作，并与一个或多个其他音频节点链接以形成音频路由图。支持具有不同通道布局的多个源，即使在单个上下文中也是如此。这种模块化设计提供了创建具有动态效果的复杂音频功能的灵活性。
- 音频节点通过它们的输入和输出连接起来，形成一个链，从一个或多个源开始，经过一个或多个节点，然后到达目的地（如果你只想可视化一些音频数据，你不必提供目的地）。一个简单的、典型的Web音频工作流看起来像这样：
  - 创建音频上下文
  - 在上下文内创建音频源（例如<audio>，振荡器或流）。
  - 创建音频效果（如混响、双二阶滤波器、平移器或压缩器节点）。
  - 选择音频的最终目的地（如用户的计算机扬声器）。
  - 将源节点连接到零个或多个效应节点，然后连接到选定的目标。
  - 注意：通道符号是一个数值，例如2.0或5.1，表示信号上可用的音频通道数。第一个数字是信号包括的全频率范围音频通道的数量。句号后的数字表示为低频效果（LFE）输出保留的通道数;这些通道通常称为低音炮。
- 每个输入或输出由一个或多个音频通道组成，它们共同代表特定的音频布局。支持任何离散通道结构，包括单声道、立体声、四声道、5.1声道等。
- 有几种方法可以获得音频
  - 声音可以直接在JavaScript中由音频节点（如振荡器）生成
  - 它可以从原始的 PCM 数据（如.WAV文件或其他支持的格式）
  - 可以从HTML媒体元素生成
  - 它可以从 WebRTC 例如网络摄像头或麦克风
# 音频数据：样本中包含什么
- 当音频信号被处理时，采样发生。采样是将连续信号转换为离散信号。换句话说，一个连续的声波，比如一个乐队现场演奏，被转换成一个数字样本序列（一个离散时间信号），允许计算机在不同的块中处理音频。
- 你可以在[Wikipedia页面](https://en.wikipedia.org/wiki/Sampling_%28signal_processing%29)采样（信号处理）上找到更多信息
# 音频缓冲区：帧、采样和通道
- AudioBuffer定义了三个参数
  - 通道的数量（单声道为1，立体声为2等）
  - 其长度，意味着缓冲器内的样本帧的数量
  - 以及采样率，即每秒播放的采样帧的数量
- 样本是单个32位浮点值，表示特定通道内每个特定时刻的音频流值（如果是立体声，则为左或右）。帧或采样帧是将在特定时刻播放的所有通道的所有值的集合：同时播放的所有通道的所有采样（两个用于立体声，六个用于5.1等）。
- 采样率是指在一秒钟内播放的样本（或帧，因为一帧的所有样本都同时播放）的数量，以Hz为单位。采样率越高，音质越好。
- 让我们来看看单声道和立体声音频缓冲器，每一个都是一秒长，速率为44100 Hz
  - 单声道缓冲器将具有44100个样本和44100帧。length将是44100
  - 立体声缓冲区将有88200个样本，但仍有44100帧。length属性仍然是44100，因为它等于帧数
- 当缓冲区播放时，您将首先听到最左边的采样帧，然后是它旁边的一个，然后是下一个，依此类推，直到缓冲区结束。在立体声的情况下，你会同时听到两个频道。采样帧非常方便，因为它们与通道数量无关，并且以精确音频操作的理想方式表示时间
- 注：要从帧计数中获得时间（以秒为单位），请将帧数除以采样率。要从采样数中获得帧数，只需将后者除以通道计数。
- 下面是几个简单的例子
```javascript
const context = new AudioContext();
const buffer = new AudioBuffer(context, {
  numberOfChannels: 2,
  length: 22050,
  sampleRate: 44100,
});
```
- 注：在数字音频中，44100Hz（交替表示为44.1 kHz）是常见的采样频率。为什么是44.1 kHz？
  - 首先，因为人耳的听觉范围大致为20 Hz至20，000 Hz。根据因此，采样率必须大于40，000 Hz。
  - 其次，信号必须在采样前进行低通滤波，否则会出现混叠。虽然理想的低通滤波器可以完美地通过低于20 kHz的频率（而不衰减它们），并完美地切断高于20 kHz的频率，但在实践中，过渡带是必要的，其中频率被部分衰减。该过渡带越宽，制作抗混叠滤波器就越容易、越经济。44.1 kHz采样频率允许2.05 kHz过渡带。
- 如果你使用上面的这个调用，你将得到一个有两个声道的立体声缓冲区，当在以44,100 Hz运行的AudioContext上播放时（非常常见，大多数正常的声卡都以这个速率运行），将持续0.5秒：22，050帧/44，100 Hz = 0.5秒
```javascript
const context = new AudioContext();
const buffer = new AudioBuffer(context, {
  numberOfChannels: 1,
  length: 22050,
  sampleRate: 22050,
});
```
- 如果您使用此调用，您将获得一个单声道缓冲区（单声道缓冲区），当在以44，100 Hz运行的AudioContext上播放时，将【自动重新采样】为44，100 Hz（因此产生44，100帧），并持续1.0秒：44，100帧/44，100 Hz = 1秒。
  - 注意：音频重现与图像重现非常相似。假设您有一个16 x 16的图像，但希望它填充32 x 32的区域。你调整它的大小（或重采样）。结果质量较差（它可能是模糊或尖锐的，取决于缩放算法），但它可以工作，调整大小的图像占用更少的空间。重采样的音频也是一样：您可以节省保存空间，但实际上无法正确再现高频内容或高音。
# 平面缓冲器与交错缓冲器
- Web Audio API使用平面缓冲区格式。左声道和右声道的存储方式如下
  - `LLLLLLLLLLLLLLLLRRRRRRRRRRRRRRRR (for a buffer of 16 frames)`
  - 这种结构在音频处理中很普遍，可以很容易地独立处理每个通道
- 另一种方法是使用交错缓冲区格式：
  - `LRLRLRLRLRLRLRLRLRLRLRLRLRLRLRLR (for a buffer of 16 frames)`
  - 这种格式是流行的存储和播放音频没有太多的处理，例如：.WAV文件或解码的MP3流
- 由于Web Audio API是为处理而设计的，因此它只公开平面缓冲区。
  - 它使用平面格式，但在将音频发送到声卡进行播放时将其转换为交错格式。相反，当API解码MP3时，它从交织格式开始，并将其转换为平面格式以进行处理。
# 音频通道
- 每个音频缓冲器可以包含不同数量的声道。大多数现代音频设备使用基本的单声道（只有一个声道）和立体声（左声道和右声道）设置。一些更复杂的设置支持环绕声设置（如四声道和5.1声道），这可以带来更丰富的声音体验，这要归功于它们的高声道数。我们通常用下表中详细列出的标准缩写来表示通道：
# 上混频和下混频`AudioNode.channelInterpretation`
- 当输入和输出的通道数不匹配时，必须进行上混合或下混合。通过将AudioNode.channelInterpretation属性设置为speakers或discrete来控制以下规则
# 可视化
- 一般来说，我们获取随时间变化的输出以产生音频可视化，通常是阅读其增益或频率数据。然后，使用图形工具，我们将获得的数据转换为可视化表示，例如图形。Web Audio API有一个AnalyserNode可用，它不会改变通过它的音频信号。此外，它输出音频数据，允许我们通过<canvas>等技术处理它。
- 您可以使用以下方法获取数据
  - AnalyserNode.getFloatFrequencyData()将当前频率数据复制到传入的Float32Array数组中
  - AnalyserNode.getByteFrequencyData()将当前频率数据复制到传入的Uint8Array（无符号字节数组）中
  - AnalyserNode.getFloatTimeDomainData()将当前波形或时域数据复制到传递给它的Float32Array数组中
  - AnalyserNode.getByteTimeDomainData()将当前波形或时域数据复制到传递给它的Uint8Array（无符号字节数组）中
# 空间化
- 音频空间化允许我们在物理空间中的某个点对音频信号的位置和行为进行建模，模拟收听者听到该音频。在Web Audio API中，空间化由PannerNode和AudioListener处理
- 平移器使用右手笛卡尔坐标来将音频源的位置描述为向量，并将其定向描述为3D方向锥。例如，对于全向源，圆锥可以相当大
- 类似地，Web Audio API使用右手笛卡尔坐标描述监听器：它们的位置作为一个向量，它们的方向作为两个方向向量，上和前。这些向量定义了听者头顶的方向和听者鼻子指向的方向。这些矢量相互垂直。
# 扇入和扇出
- 在音频术语中，扇入描述了ChannelMergerNode接收一系列单声道输入源并输出单个多声道信号的过程
- 扇出描述了相反的过程，其中ChannelSplitterNode采用多声道输入源并输出多个单声道输出信号
- 

