# Web Audio API
- [文档地址](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API)
- Web Audio API为控制Web上的音频提供了一个功能强大的通用系统，允许开发人员选择音频源，为音频添加效果，创建音频可视化，应用空间效果（如平移）等等。
# 概念和使用
- Web Audio API涉及在音频上下文中处理音频操作，并且被设计为允许模块化路由。基本的音频操作是通过音频节点来执行的，这些音频节点被链接在一起以形成音频路由图。即使在一个上下文中，也支持多个源（具有不同类型的通道布局）。这种模块化设计提供了创建具有动态效果的复杂音频功能的灵活性。
- 音频节点通过它们的输入和输出链接成链和简单的网络。它们通常从一个或多个源开始。声源以非常小的时间片提供声音强度（样本）的阵列，通常每秒数万个。这些可以通过数学计算（如OscillatorNode），或者它们可以是来自声音/视频文件（如AudioBufferSourceNode和MediaElementAudioSourceNode）和音频流（MediaStreamAudioSourceNode）的记录。事实上，声音文件只是声音强度本身的记录，它来自麦克风或电子仪器，并混合成一个单一的，复杂的波。
- 这些节点的输出可以链接到其他节点的输入，这些节点将这些声音样本流混合或修改为不同的流。一个常见的修改是将样本乘以一个值，使它们更响亮或更安静（如GainNode的情况）。一旦声音已经被充分处理以达到预期效果，它就可以被链接到目的地（BaseAudioContext.destination）的输入，目的地（undefined）将声音发送到扬声器或耳机。只有当用户应该听到音频时，最后一个连接才是必要的
- 一个简单的、典型的Web音频工作流看起来像这样：
    - 1、创建音频上下文
    - 2、在上下文内，创建音频源-例如<audio>、振荡器、流
    - 3、创建效果节点，如混响，双二阶滤波器，平移器，压缩器
    - 4、选择音频的最终目的地，例如系统扬声器
    - 5、将源连接到效果，并将效果连接到目的地
- 定时控制具有高精度和低延迟，允许开发人员编写准确响应事件的代码，并且即使在高采样率下也能够针对特定样本。因此，鼓机和定序器等应用是触手可及的。
- Web Audio API还允许我们控制音频的空间化方式。使用基于源-收听者模型的系统，其允许控制平移模型并处理由移动源（或移动收听者）引起的距离引起的衰减。
# 目标受众
- 对于那些不熟悉音频或音乐术语的人来说，Web Audio API似乎令人生畏，并且由于它包含了大量功能，如果您是开发人员，则很难开始使用。
- 它可以用于将音频整合到您的网站或应用程序中，通过提供表单上的听觉反馈等氛围。然而，它也可以用来创建高级交互式工具。考虑到这一点，它适合开发人员和音乐家。
    - [表单校验发出声音](https://css-tricks.com/form-validation-web-audio/)
# 音频API接口
- Web Audio API有许多接口和相关事件，我们将其分为九类功能。
- 通用音频图形定义
    - AudioContext接口表示由链接在一起的音频模块构建的音频处理图，每个音频模块由AudioNode表示。音频上下文控制其包含的节点的创建以及音频处理或解码的执行。你需要在做任何其他事情之前创建一个AudioContext，因为一切都发生在上下文中。
    - AudioNode接口表示音频处理模块，如音频源（例如，HTML<audio>或<video>元素）、音频目的地、中间处理模块（例如，过滤器，如BiquadFilterNode，或音量控制，如GainNode）。
    - AudioParam接口表示与音频相关的参数，如AudioNode接口中的一个。它可以设置为特定值或值的更改，并且可以计划在特定时间发生并遵循特定模式。
    - 为一组AudioParam接口提供类似于map的接口，这意味着它提供了方法forEach()、get()、has()、keys()和values()，以及size属性。
    - BaseAudioContext接口用作在线和离线音频处理图的基本定义，分别由AudioContext和OfflineAudioContext表示。你不会直接使用BaseAudioContext-你会通过这两个继承接口之一来使用它的特性。
    - 当播放因到达媒体结尾而停止时，将触发ended事件
- 定义音频源
    - AudioScheduledSourceNode是几种类型的音频源节点接口的父接口。是一个AudioNode。
    - OscillatorNode接口表示周期性波形，例如正弦波或三角波。它是一个AudioNode音频处理模块，可以产生给定频率的波。
    - AudioBuffer接口表示驻留在内存中的短音频资产，使用BaseAudioContext.decodeAudioData方法从音频文件创建，或使用BaseAudioContext.createBuffer使用原始数据创建。一旦解码成这种形式，音频就可以放入AudioBufferSourceNode中。
    - AudioBufferSourceNode接口表示由存储在AudioBuffer中的存储器内音频数据组成的音频源。它是一个AudioNode，充当音频源。
    - MediaElementAudioSourceNode接口表示由HTML<audio>或<video>元素组成的音频源。它是一个AudioNode，充当音频源。
    - MediaStreamAudioSourceNode接口表示由MediaStream组成的音频源（例如网络摄像头、麦克风或从远程计算机发送的流）。如果流上存在多个音轨，则使用其id按词典顺序（按字母顺序）排在第一位的音轨。它是一个AudioNode，充当音频源。
    - MediaStreamTrackAudioSourceNode类型的节点表示其数据来自MediaStreamTrack的音频源。当使用createMediaStreamTrackSource()方法创建节点时，您可以指定要使用的轨道。这提供了比MediaStreamAudioSourceNode更多的控制。
- 定义音频效果过滤器
    - BiquadFilterNode接口代表一个简单的低阶滤波器。它是一个AudioNode，可以代表不同类型的滤波器，音调控制设备或图形均衡器。BiquadFilterNode总是只有一个输入和一个输出。
    - ConvolverNode接口是在给定的AudioNode上执行线性卷积的AudioBuffer，并且通常用于实现混响效果。
    - DelayNode接口代表一个延迟线;AudioNode音频处理模块，它在输入数据到达和传播到输出之间产生延迟。
    - DynamicsCompressorNode接口提供压缩效果，降低信号最大音量部分的音量，以帮助防止同时播放和多路复用多个声音时可能发生的削波和失真。
    - GainNode界面表示音量变化。它是一个AudioNode音频处理模块，在输入数据传播到输出之前，将给定的增益应用于输入数据。
    - WaveShaperNode接口表示非线性失真器。这是一个AudioNode，使用曲线应用波形失真的信号。除了明显的失真效果外，它还经常用于为信号添加温暖的感觉。
    - PeriodicWave描述可用于整形OscillatorNode输出的周期性波形
    - IIRFilterNode实现通用无限脉冲响应（IIR）滤波器;这种类型的滤波器也可用于实现音调控制设备和图形均衡器。
- 定义音频目标
    - AudioDestinationNode接口表示音频源在给定上下文中的最终目的地-通常是设备的扬声器
    - MediaStreamAudioDestinationNode接口表示由WebRTCMediaStream和单个AudioMediaStreamTrack组成的音频目的地，其可以以与从MediaStream获得的getUserMedia()类似的方式使用。它是一个AudioNode，充当音频目的地。
- 数据分析和可视化
    - AnalyserNode接口表示能够提供实时频域和时域分析信息的节点，用于数据分析和可视化。
- 分割和合并音频通道
    - ChannelSplitterNode接口将音频源的不同通道分离为一组单声道输出
    - ChannelMergerNode接口将不同的单声道输入重新合并为单个输出。每个输入将用于填充输出的一个通道。
- 音频空间化
    - AudioListener接口表示收听音频空间化中使用的音频场景的唯一人的位置和定向。
    - PannerNode界面代表音频源信号在3D空间中的位置和行为，允许您创建复杂的平移效果。
    - StereoPannerNode接口表示可用于向左或向右平移音频流的简单立体声平移器节点
- JavaScript中的音频处理
    - 使用音频工作小程序，可以定义用JavaScript或WebAssembly编写的自定义音频节点。音频工作包实现了Worklet接口，这是Worker接口的轻量级版本
    - AudioWorklet接口可通过AudioContext对象的audioWorklet获得，并允许您将模块添加到音频工作程序以脱离主线程执行。
    - AudioWorkletNode接口表示嵌入到音频图中的AudioNode，并且可以将消息传递到对应的AudioWorkletProcessor。
    - AudioWorkletProcessor接口表示在AudioWorkletGlobalScope中运行的音频处理代码，其直接生成、处理或分析音频，并且可以将消息传递到对应的AudioWorkletNode。
    - AudioWorkletGlobalScope接口是一个WorkletGlobalScope派生的对象，表示在其中运行音频处理脚本的工作者上下文;它被设计为在工作线程中而不是在主线程中直接使用JavaScript来生成、处理和分析音频数据。
- 脚本处理器节点
    - 在定义音频worklet之前，Web Audio API使用ScriptProcessorNode进行基于JavaScript的音频处理。因为代码在主线程中运行，所以它们的性能很差。由于历史原因，ScriptProcessorNode被保留，但被标记为已弃用。
    - ScriptProcessorNode接口允许使用JavaScript生成、处理或分析音频。它是一个AudioNode音频处理模块，连接到两个缓冲器，一个包含当前输入，一个包含输出。每次输入缓冲区包含新数据时，都会向对象发送一个实现AudioProcessingEvent接口的事件，并且事件处理程序在输出缓冲区填充完数据后终止。
    - 当Web音频APIaudioprocess的输入缓冲区准备好被处理时，激发ScriptProcessorNode事件。
    - AudioProcessingEvent表示当ScriptProcessorNode输入缓冲器准备好被处理时发生的事件。
- 离线/后台音频处理
    - 可以在后台非常快速地处理/渲染音频图-将其渲染到AudioBuffer而不是设备的扬声器-具有以下内容。
    - OfflineAudioContext接口是表示从链接在一起的AudioContext构建的音频处理图的AudioNode接口。与标准的AudioContext相比，OfflineAudioContext并不真正渲染音频，而是在缓冲区中尽可能快地生成音频。
    - complete事件在OfflineAudioContext渲染终止时触发
    - OfflineAudioCompletionEvent表示当OfflineAudioContext的处理终止时发生的事件。complete事件使用此接口
# 指南和教程
- [高级技术：创建和排序音频](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Advanced_techniques)
    - 在本教程中，我们将介绍声音的创建和修改，以及时间和调度。我们将介绍样品加载、包络、滤波器、波表和频率调制。如果您熟悉这些术语，并且正在寻找Web Audio API应用程序的介绍，那么您来对地方了。
- [使用AudioWorklet进行后台音频处理](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_AudioWorklet)
    - 本文介绍了如何创建音频worklet处理器并在Web音频应用程序中使用它。
- [Web Audio API背后的基本概念](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API)
    - 本文解释了Web Audio API的功能如何工作的背后的一些音频理论，以帮助您在设计应用如何路由音频时做出明智的决策。如果你还不是一个声音工程师，它将给给予你足够的背景，以了解为什么网络音频API的工作方式。
- [使用ConstantSourceNode控制多个参数](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Controlling_multiple_parameters_with_ConstantSourceNode)
    - 本文演示了如何使用ConstantSourceNode将多个参数链接在一起，使它们共享相同的值，可以通过设置ConstantSourceNode.offset参数的值来更改。
- [示例和教程：简单的合成键盘](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Simple_synth)
    - 本文介绍了一个可以使用鼠标进行操作的视频键盘的代码和工作演示。键盘允许您在标准波形和一个自定义波形之间切换，并且您可以使用键盘下方的音量滑块控制主增益。此示例使用以下Web API接口：AudioContext、OscillatorNode、PeriodicWave和GainNode。
- [用于分析Web音频使用情况的工具](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Tools)
    - 在处理Web音频API代码时，您可能会发现需要一些工具来分析您创建的节点图或调试您的工作。本文讨论了可以帮助您实现这一点的工具。
- [使用IIR滤波器](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_IIR_filters)
    - Web Audio API的IIRFilterNode接口是实现通用无限脉冲响应（IIR）滤波器的AudioNode处理器;这种类型的滤波器可以用于实现音调控制设备和图形均衡器，并且可以指定滤波器响应参数，从而可以根据需要对其进行调谐。本文将介绍如何实现它，并在一个简单的示例中使用它。
- [使用Web音频API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API)
    - 让我们来看看如何开始使用Web Audio API。我们将简要介绍一些概念，然后研究一个简单的Boombox示例，该示例允许我们加载音轨、播放和暂停音轨，并更改其音量和立体声平移。
- [使用Web Audio API进行可视化](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Visualizations_with_Web_Audio_API)
    - Web Audio API最有趣的功能之一是能够从音频源中提取频率、波形和其他数据，然后可以使用这些数据创建可视化效果。本文解释了如何实现，并提供了几个基本的用例。
- [Web音频API最佳实践](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Best_practices)
    - 在编写创造性的代码时，没有严格的正确或错误的方法。只要考虑到安全性、性能和可访问性，您就可以适应自己的风格。在本文中，我们将分享一些最佳实践-使用Web Audio API的指导方针、提示和技巧。
- [Web音频空间化基础](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Web_audio_spatialization_basics)
    - 好像它的各种各样的声音处理（和其他）选项还不够，Web Audio API还包括一些工具，允许您在收听者围绕声源移动时模拟声音的差异，例如在3D游戏中围绕声源移动时进行平移。 这方面的官方术语是空间化，本文将介绍如何实现这样一个系统的基础知识。
# Examples示例
- 您可以在GitHub上的[webaudio-example repo](https://github.com/mdn/webaudio-examples/)中找到一些示例。
